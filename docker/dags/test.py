import boto3
import os
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

# Configuration MinIO
MINIO_URL = "http://localhost:9001"  # Port modifiÃ© Ã  9001
ACCESS_KEY = "minioadmin"
SECRET_KEY = "minioadmin"
BUCKET_NAME = "test"
FILE_NAME = "test_file.txt"

# DÃ©finir les chemins des fichiers locaux
LOCAL_DIR = os.path.join(os.path.expanduser("~"), "Documents", "MSPR")  # RÃ©pertoire oÃ¹ le fichier sera crÃ©Ã©
LOCAL_PATH = os.path.join(LOCAL_DIR, FILE_NAME)
DOWNLOAD_PATH = os.path.join(LOCAL_DIR, "minio_downloaded.txt")

# Fonction d'upload vers MinIO
def upload_to_minio():
    """Ã‰crit un fichier sur MinIO."""
    print(f"DÃ©but de l'upload vers MinIO Ã  {datetime.now()}")

    # VÃ©rifier que le dossier existe
    os.makedirs(LOCAL_DIR, exist_ok=True)

    # CrÃ©ation du fichier local
    with open(LOCAL_PATH, "w") as f:
        f.write("Hello MinIO! Test avec Airflow.")

    # Connexion Ã  MinIO
    s3_client = boto3.client(
        "s3",
        endpoint_url=MINIO_URL,
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
    )

    # Upload du fichier
    try:
        s3_client.upload_file(LOCAL_PATH, BUCKET_NAME, FILE_NAME)
        print(f"âœ… Fichier {FILE_NAME} uploadÃ© sur {BUCKET_NAME}")
    except Exception as e:
        print(f"ğŸš¨ Erreur lors de l'upload : {e}")

# Fonction de lecture depuis MinIO
def read_from_minio():
    """Lit un fichier depuis MinIO."""
    print(f"DÃ©but de la lecture depuis MinIO Ã  {datetime.now()}")

    s3_client = boto3.client(
        "s3",
        endpoint_url=MINIO_URL,
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
    )

    # VÃ©rifier que le dossier existe pour le tÃ©lÃ©chargement
    os.makedirs(LOCAL_DIR, exist_ok=True)

    # TÃ©lÃ©chargement du fichier
    try:
        s3_client.download_file(BUCKET_NAME, FILE_NAME, DOWNLOAD_PATH)
        with open(DOWNLOAD_PATH, "r") as f:
            content = f.read()
        print(f"ğŸ“„ Contenu du fichier tÃ©lÃ©chargÃ© : {content}")
        return content
    except Exception as e:
        print(f"ğŸš¨ Erreur lors du tÃ©lÃ©chargement : {e}")
        return None

# Fonction pour lister les objets dans le bucket
def list_objects_in_bucket():
    """Liste tous les objets dans un bucket."""
    s3_client = boto3.client(
        "s3",
        endpoint_url=MINIO_URL,
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY,
    )

    try:
        response = s3_client.list_objects_v2(Bucket=BUCKET_NAME)

        if response.get('Contents'):
            print(f"Objets dans le bucket {BUCKET_NAME}:")
            for obj in response['Contents']:
                print(f" - {obj['Key']} ({obj['Size']} bytes)")
        else:
            print(f"ğŸš¨ Le bucket {BUCKET_NAME} est vide")
    except Exception as e:
        print(f"ğŸš¨ Erreur lors de la liste des objets : {e}")

# DÃ©finition du DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 3, 11),
    "retries": 1,
    "retry_delay": timedelta(minutes=1),
}

dag = DAG(
    "minio_dag",
    default_args=default_args,
    description="Un DAG pour Ã©crire et lire depuis MinIO",
    schedule_interval=None,
    catchup=False,
)

# TÃ¢ches
upload_task = PythonOperator(
    task_id="upload_to_minio",
    python_callable=upload_to_minio,
    dag=dag,
)

read_task = PythonOperator(
    task_id="read_from_minio",
    python_callable=read_from_minio,
    dag=dag,
)

list_task = PythonOperator(
    task_id="list_objects_in_bucket",
    python_callable=list_objects_in_bucket,
    dag=dag,
)

# DÃ©pendances
upload_task >> list_task >> read_task
